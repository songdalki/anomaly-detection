{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### DATAFRAME 생성 ##################################\n",
    "# 파일 경로 및 파일 개수 설정\n",
    "file_prefix = \"case_\"\n",
    "file_suffix = \"_raw_data.csv\"\n",
    "num_files = 500\n",
    "\n",
    "# 파일을 저장할 빈 리스트 생성\n",
    "dfs = []\n",
    "\n",
    "# 파일을 읽어 DataFrame으로 변환하여 리스트에 추가\n",
    "for i in range(1, num_files + 1):\n",
    "    file_path = f\"{file_prefix}{i}{file_suffix}\"\n",
    "    \n",
    "    # CSV 파일을 DataFrame으로 읽어오기\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 리스트에 DataFrame 추가\n",
    "    dfs.append(df)\n",
    "\n",
    "# 'Value' column을 TensorFlow Dataset으로 변환\n",
    "features =  [tf.convert_to_tensor(tf.constant(df['Value'], dtype=tf.float32)) for df in dfs]\n",
    "\n",
    "# Feature normalization using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = [scaler.fit_transform(feature.numpy().reshape(-1, 1)).flatten() for feature in features]\n",
    "\n",
    "# 각 DataFrame에 대한 1/0 레이블 생성\n",
    "labels = [1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,1,0,1,0,1,0,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,0,1,0,1,0,0,1,1,1,0,0,0,1,0,0,1,1,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,1,1,0,0,1,1,0,1,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,1,0,1,0,1,0,1,0,0,0,1,1,0,0,1,0,1,1,0,1,0,1,0,1,1,0,0,0,0,0,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,1,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,1,0,1,0,1,0,0,0,1,0,1,0,1,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,0,1,1,1,0,1,1,0,1,1,0,1,1,0,0,1,0,1,1,1,0,1,0,1,1,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,0,1,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0]\n",
    "\n",
    "# zero-padding을 적용할 최대 길이 설정 (적절한 값으로 설정해야 함)\n",
    "max_length = 1000  \n",
    "\n",
    "# zero-padding 적용\n",
    "#padded_features = [tf.keras.preprocessing.sequence.pad_sequences([feature.numpy()], maxlen=max_length, padding='post')[0] for feature in features]\n",
    "padded_features = [tf.keras.preprocessing.sequence.pad_sequences([normalized_features.numpy()], maxlen=max_length, padding='post')[0] for normalized_features in features]\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((padded_features, labels))\n",
    "\n",
    "# 데이터셋을 train 및 validation으로 분리\n",
    "train_size = int(0.8 * len(padded_features))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "\n",
    "\n",
    "# Define the TensorBoard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
    "\n",
    "#하이퍼파라미터 정보\n",
    "hypers = [\n",
    "    [0.00001, #learning rate\n",
    "    16,    #batch size\n",
    "    2000],  #epoch size\n",
    "    [0.00002, #learning rate\n",
    "    16,    #batch size\n",
    "    2000],  #epoch size\n",
    "    [0.00004, \n",
    "    16,    \n",
    "    2000],\n",
    "      \n",
    "    [0.00002, \n",
    "    32,    \n",
    "    2000],  \n",
    "    [0.00004, \n",
    "    32,    \n",
    "    2000],  \n",
    "    [0.00006, \n",
    "    32,    \n",
    "    2000],  \n",
    "    \n",
    "    [0.00002, \n",
    "    100,    \n",
    "    2000],  \n",
    "    [0.00004,\n",
    "    50,\n",
    "    2000],\n",
    "    [0.00006,\n",
    "    50,\n",
    "    2000],\n",
    "    [0.00006,\n",
    "    100,\n",
    "    2000],\n",
    "    [0.00006,\n",
    "    200,\n",
    "    2000],\n",
    "    [0.00002,\n",
    "    100,\n",
    "    2000],\n",
    "    [0.00002,\n",
    "    150,\n",
    "    2000],\n",
    "    [0.00002,\n",
    "    200,\n",
    "    2000],\n",
    "    [0.001,\n",
    "    200,\n",
    "    2000],\n",
    "    [0.01,\n",
    "    200,\n",
    "    2000],\n",
    "    [0.1,\n",
    "    200,\n",
    "    2000],\n",
    "    [0.001,\n",
    "    100,\n",
    "    2000],\n",
    "    [0.01,\n",
    "    100,\n",
    "    2000],\n",
    "    [0.1,\n",
    "    100,\n",
    "    2000]\n",
    "]\n",
    "\n",
    "layerShape = [\n",
    "    [128,320,16],\n",
    "    [128,320,32],\n",
    "    [128,320,64],\n",
    "    [128,320,128],\n",
    "    [64,320,16],\n",
    "    [32,320,32],\n",
    "    [16,320,64],\n",
    "    [8,320,128],\n",
    "    [64,640,16],\n",
    "    [32,800,32],\n",
    "    [16,160,64],\n",
    "    [8,80,128],\n",
    "]\n",
    "# 1D CNN 모델 정의\n",
    "\n",
    "\n",
    "for layer in layerShape:\n",
    "    # kernel = layer[0]\n",
    "    # kernel_size = layer[1]\n",
    "    # dense_size = layer[2]\n",
    "    for hyper in hypers:\n",
    "        #epoch\n",
    "        learning_rate = hyper[0]\n",
    "        batch_size = hyper[1]\n",
    "        epoch = hyper[2]\n",
    "        \n",
    "        # 데이터셋을 배치로 변환\n",
    "        train_dataset_local = train_dataset.batch(batch_size)\n",
    "        val_dataset_local = val_dataset.batch(batch_size)\n",
    "        \n",
    "        \n",
    "        model = Sequential([\n",
    "        Conv1D(128, 320, activation='relu', input_shape=(max_length, 1)),\n",
    "        MaxPooling1D(pool_size=6),#그냥 MaxPooling1D로 바꿔보자\n",
    "        Conv1D(32, 64, activation='relu', input_shape=(max_length, 1)),\n",
    "        MaxPooling1D(pool_size=3),#그냥 MaxPooling1D로 바꿔보자\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        custom_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # 모델 컴파일\n",
    "        model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # 모델 저장\n",
    "        \n",
    "        \n",
    "\n",
    "        # 모델 요약 확인 (기존))\n",
    "        #model.summary()\n",
    "        # 모델 요약 확인 (추가된 것)\n",
    "        model_summary = []\n",
    "        model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "        model_summary = \"\\n\".join(model_summary)\n",
    "        # 테이블에 저장할 정보\n",
    "        info_table = pd.DataFrame({\n",
    "            \"Parameter\": [\"File Prefix\", \"File Suffix\", \"Number of Files\", \"Max Length\", \"Batch Size\", \"Learning Rate\", \"Epochs\"],\n",
    "            \"Value\": [file_prefix, file_suffix, num_files, max_length, batch_size, custom_optimizer.lr.numpy(), epoch]  \n",
    "        })\n",
    "\n",
    "        # 모델 학습\n",
    "        history = model.fit(train_dataset_local, epochs=epoch, validation_data=val_dataset_local, callbacks=[tensorboard_callback])\n",
    "\n",
    "        # 결과 테이블에 저장할 정보\n",
    "        result_table = pd.DataFrame({\n",
    "            \"Metric\": [\"Train Loss\", \"Train Accuracy\", \"Validation Loss\", \"Validation Accuracy\"],\n",
    "            \"Value\": [history.history['loss'][-1], history.history['accuracy'][-1], history.history['val_loss'][-1], history.history['val_accuracy'][-1]]\n",
    "        })\n",
    "\n",
    "        # 출력\n",
    "        print(\"Model Summary:\")\n",
    "        print(model_summary)\n",
    "        print(\"\\nInformation Table:\")\n",
    "        print(info_table)\n",
    "        print(\"\\nResult Table:\")\n",
    "        print(result_table)\n",
    "        \n",
    "        #파일 출력\n",
    "        result_table.to_csv(f'result_table_lr{learning_rate}_bs{batch_size}_ep{epoch}.csv', index=False)\n",
    "        model.save_weights(f\"model_weights_lr{learning_rate}_bs{batch_size}_ep{epoch}.hdf5\")\n",
    "\n",
    "        # 모델 평가\n",
    "        loss, accuracy = model.evaluate(val_dataset_local)\n",
    "        print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "        # 학습 곡선 시각화\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f\"lr{learning_rate}_bs{batch_size}_ep{epoch}\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
